{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "from sklearn.svm import OneClassSVM\n",
    "from numpy.random import seed\n",
    "from keras.layers import Input, Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras import regularizers\n",
    "from keras.models import model_from_json\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_df = pd.read_csv('../input/datasetone-year-compiledcsv/One_year_compiled.csv')\n",
    "main_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Heatmap of correlations from -1 to 1\n",
    "sns.heatmap(main_df.corr(), vmin = -1, vmax = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_df = pd.read_csv('../input/datasetone-year-compiledcsv/One_year_compiled.csv')\n",
    "# Dropping nonsense columns for this proposal                            (axis=1) = columns\n",
    "main_df = main_df.drop(['day', 'hour', 'sample_Number', 'month', 'timestamp'], axis=1)\n",
    "# Flipping column values\n",
    "main_df['pCut::Motor_Torque'] = main_df['pCut::Motor_Torque'] * -1\n",
    "# heatmap\n",
    "sns.heatmap(main_df.corr(), vmin = -1, vmax = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def handle_non_numeric(df):\n",
    "    # Values in each column for each column\n",
    "    columns = df.columns.values\n",
    "    \n",
    "    for column in columns:\n",
    "        \n",
    "        # Dictionary with each numerical value for each text\n",
    "        text_digit_vals = {}\n",
    "        \n",
    "        # Receives text to convert to a number\n",
    "        def convert_to_int(val):\n",
    "            \n",
    "            # Returns respective numerical value for class\n",
    "            return text_digit_vals[val]\n",
    "        \n",
    "        # If values in columns are not float or int\n",
    "        if df[column].dtype != np.int64 and df[column].dtype != np.float64:\n",
    "            \n",
    "            # Gets values from current column\n",
    "            column_contents = df[column].values.tolist()\n",
    "            \n",
    "            # Gets unique values from current column\n",
    "            unique_elements = set(column_contents)\n",
    "            \n",
    "            # Classification starts at 0\n",
    "            x = 0\n",
    "            \n",
    "            for unique in unique_elements:\n",
    "                \n",
    "                # Adds the class value for the text in dictionary, if it's not there\n",
    "                if unique not in text_digit_vals:\n",
    "                    text_digit_vals[unique] = x\n",
    "                    x += 1\n",
    "                    \n",
    "            # Maps the numerical values to the text values in columns\n",
    "            df[column] = list(map(convert_to_int, df[column]))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grabbing the entire dataset\n",
    "main_df = pd.read_csv('../input/datasetone-year-compiledcsv/One_year_compiled.csv')\n",
    "# Dropping columns with unwanted/irrelevant info for the algorithm\n",
    "main_df = main_df.drop(['day', 'hour', 'sample_Number', 'month', 'timestamp'], axis=1)\n",
    "# Transforming modes into classified data\n",
    "main_df = handle_non_numeric(main_df)\n",
    "\n",
    "# Passing our dataframe as our features\n",
    "X = main_df\n",
    "\n",
    "# Defining preprocessor for the data\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "# Preprocessing\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns = X.columns, index = X.index)\n",
    "\n",
    "# Scaling\n",
    "X = preprocessing.scale(X)\n",
    "# Splitting the feature data for training data. First 200.000 rows.\n",
    "X_train = X[:200000]\n",
    "\n",
    "#Creating a fitting OneClass SVM\n",
    "ocsvm = OneClassSVM(nu = 0.25, gamma = 0.05)\n",
    "ocsvm.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = main_df.copy()\n",
    "df['anormaly'] = pd.Series(ocsvm.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving dataframe\n",
    "df.to_csv('Labled_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading into dataframe\n",
    "df = pd.read_csv('../input/created/Labled_df.csv', index_col = 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting labled groups\n",
    "scat_1 = df.groupby('anormaly').get_group(1)\n",
    "scat_0 = df.groupby('anormaly').get_group(-1)\n",
    "\n",
    "# Plot size\n",
    "plt.subplots(figsize = (15, 7))\n",
    "\n",
    "# Plot group 1 -labeled, color green, point size 1\n",
    "plt.plot(scat_1.index, scat_1['pCut::Motor_Torque'], 'g.', markersize = 1)\n",
    "\n",
    "# Plot group 0 -labeled, color red, point size 1\n",
    "plt.plot(scat_0.index, scat_0['pCut::Motor_Torque'], 'r.', markersize = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a dataframe for the score of each data sample\n",
    "score = pd.DataFrame()\n",
    "# Returning scores for the dataset\n",
    "score['score'] = ocsvm.score_samples(X)\n",
    "\n",
    "# Plot size\n",
    "plt.subplots(figsize = (15, 7))\n",
    "# Plotting\n",
    "score['score'].plot()\n",
    "# Saving score dataframe\n",
    "score.to_csv('SVM_Score.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15,7))\n",
    "\n",
    "((score['score'].rolling(20000).mean()) * -1).plot(ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (15, 7))\n",
    "plt.plot(score.index, score['score'], 'r.', markersize = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preparing features for training and future prediction\n",
    "main_df = pd.read_csv('../input/datasetone-year-compiledcsv/One_year_compiled.csv')\n",
    "main_df = main_df.drop(['day', 'hour', 'sample_Number', 'month', 'timestamp'], axis = 1)\n",
    "main_df = handle_non_numeric(main_df)\n",
    "X = main_df\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns = X.columns, index = X.index)\n",
    "\n",
    "X = preprocessing.scale(X)\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "# Percentage of the data that will be considered healthy condition\n",
    "train_percentage = 0.15\n",
    "# Integer value for the slice that will be considered healthy condition\n",
    "train_size = int(len(main_df.index) * train_percentage)\n",
    "# Grabbing slice for training data\n",
    "X_train = X[:train_size]\n",
    "\n",
    "# Defining KMeans with 1 cluster\n",
    "kmeans = KMeans(n_clusters = 1)\n",
    "# Fitting the algorithm\n",
    "kmeans.fit(X_train)\n",
    "\n",
    "# Creating a copy of the main dataset\n",
    "k_anomaly = main_df.copy()\n",
    "\n",
    "# Dataframe now will recieve the distance of each data sample from the cluster\n",
    "k_anomaly = pd.DataFrame(kmeans.transform(X))\n",
    "\n",
    "# Saving cluster distance into csv file\n",
    "k_anomaly.to_csv('KM_Distance.csv')\n",
    "\n",
    "# Plot\n",
    "plt.subplots(figsize = (15, 7))\n",
    "\n",
    "plt.plot(k_anomaly.index, k_anomaly[0], 'g', markersize = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preparing data for training\n",
    "main_df = pd.read_csv('../input/datasetone-year-compiledcsv/One_year_compiled.csv')\n",
    "main_df = main_df.drop(['day', 'hour', 'sample_Number', 'month', 'timestamp'], axis = 1)\n",
    "main_df = handle_non_numeric(main_df)\n",
    "X = main_df\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns = X.columns, index = X.index)\n",
    "\n",
    "X = preprocessing.scale(X)\n",
    "\n",
    "train_percentage = 0.15\n",
    "train_size = int(len(main_df.index) * train_percentage)\n",
    "\n",
    "X_train = X[:train_size]\n",
    "#-----------------------------------------------------------------------------------------\n",
    "\n",
    "# Seed for random batch validation and training\n",
    "seed(10)\n",
    "\n",
    "#Elu activation function\n",
    "act_func = 'elu'\n",
    "\n",
    "# Input layer\n",
    "model = Sequential()\n",
    "\n",
    "# First hidden layer, connected to input vector X\n",
    "model.add(Dense(50, activation = act_func,\n",
    "               kernel_initializer = 'glorot_uniform',\n",
    "               kernel_regularizer = regularizers.l2(0.0),\n",
    "               input_shape = (X_train.shape[1],)\n",
    "               )\n",
    "         )\n",
    "# Second hidden layer\n",
    "model.add(Dense(10, activation = act_func,\n",
    "               kernel_initializer = 'glorot_uniform'))\n",
    "# Third hidden layer\n",
    "model.add(Dense(50, activation = act_func,\n",
    "               kernel_initializer = 'glorot_uniform'))\n",
    "\n",
    "# Input layer\n",
    "model.add(Dense(X_train.shape[1],\n",
    "               kernel_initializer = 'glorot_uniform'))\n",
    "\n",
    "# Loss function and Optimizer choice\n",
    "model.compile(loss = 'mse', optimizer = 'adam')\n",
    "\n",
    "# Train model for 50 epochs, batch size of 200\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 200\n",
    "\n",
    "# Grabbing validation and training loss over epochs\n",
    "history = model.fit(np.array(X_train), np.array(X_train),\n",
    "                   batch_size = BATCH_SIZE,\n",
    "                   epochs = NUM_EPOCHS,\n",
    "                   validation_split = 0.1,\n",
    "                   verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (15, 7))\n",
    "\n",
    "plt.plot(history.history['loss'], 'b', label = 'Training loss')\n",
    "plt.plot(histiry.history['val_loss'], 'r', label = 'Validation loss')\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss, [mse]')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reconstructing train data\n",
    "X_pred = model.predict(np.array(X_train))\n",
    "\n",
    "# Creating dataframe for reconstructed data\n",
    "X_pred = pd.DataFrame(X_pred, columns = main_df.columns)\n",
    "X_pred.index = pd.DataFrame(X_train).index\n",
    "\n",
    "# Dataframe to get the difference  of predicted data and real data\n",
    "scored = pd.DataFrame(index = pd.DataFrame(X_train).index)\n",
    "# Returning the mean of the loss for each column\n",
    "scored['Loss_mae'] = np.mean(np.abs(X_pred-X_train), axis = 1)\n",
    "\n",
    "#plot\n",
    "plt.subplots(figsize = (15, 7))\n",
    "sns.displot(scored['Loss_mae'], bins=15, kde=True, color='blue');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reconstructing full data\n",
    "X_pred = model.predict(np.array(X))\n",
    "X_pred = pd.DataFrame(X_pred, columns = main_df.columns)\n",
    "X_pred.index = pd.DataFrame(X).index\n",
    "\n",
    "# Returning mean of the losses for each column and putting it in a dataframe\n",
    "scored = pd.DataFrame(index = pd.DataFrame(X).index)\n",
    "scored['Loss_mae'] = np.mean(np.abs(X_pred-X), axis = 1)\n",
    "\n",
    "# Plot size\n",
    "plt.subplots(figsize = (15, 7))\n",
    "\n",
    "# Saving dataframe\n",
    "scored.to_scv('AutoEncoder_loss.csv')\n",
    "\n",
    "# Plot\n",
    "plt.plot(scored['Loss_mse'], 'b', label = 'Prediction Loss')\n",
    "\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Loss, [mse]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot size\n",
    "plt.subplots(figsize = (15, 7))\n",
    "# Reading loss csv file\n",
    "enc_loss = pd.read_csv('../input/created/AutoEncoder_loss.csv')\n",
    "# Plot\n",
    "plt.plot(enc_loss.index, enc_loss['Loss_mae'], 'g.', markersize = 1, label = \"AutoEncoder Loss\")\n",
    "# Labels and legends\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.xlabel('Sample')\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(figsize = (15, 7))\n",
    "k_anomaly = pd.read_csv('../input/created/KM_Distance.csv')\n",
    "plt.plot(k_anomaly.index, k_anomaly['0'], 'g.', markersize = 1, label = \"KM cluster Distance\")\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.xlabel('Sample')\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(figsize = (15, 7))\n",
    "k_anomaly = pd.read_csv('../input/created/SVM_Score.csv')\n",
    "plt.plot(score.index, score['score'], 'g.', markersize = 1, label = \"OCSVM score\")\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.xlabel('Sample')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot size\n",
    "plt.subplots(figsize = (15, 7))\n",
    "\n",
    "# Reading each scoring csv file\n",
    "k_anomaly = pd.read_csv('../input/created/KM_Distance.csv')\n",
    "score = pd.read_csv('../input/created/SVM_Score.csv')\n",
    "enc_loss = pd.read__csv('../input/created/AutoEncoder_loss.csv')\n",
    "\n",
    "# Scaling data for visualization\n",
    "k_distance = k_anomaly/k_anomaly.max()\n",
    "svm_score = (score/score.max()) * -1\n",
    "\n",
    "\n",
    "plt.plot(enc_loss.index, enc_loss['Loss_mae'], label = \"AutoEncoder Loss\")\n",
    "plt.plot(k_distance.index, k_distance['0'], label = \"Kmeans Euclidean Dist\")\n",
    "plt.plot(svm_score.index, svm_score['score'], label = \"OCSVM score\")\n",
    "\n",
    "plt.gca().legend(('AutoEncoder Loss', 'OCSVM score * -1', 'Kmeans Euclidean Dist'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading score files\n",
    "k_anomaly = pd.read_csv('../input/created/KM_Distance.csv')\n",
    "score = pd.read_csv('../input/created/SVM_Score.csv')\n",
    "enc_loss = pd.read__csv('../input/created/AutoEncoder_loss.csv')\n",
    "\n",
    "# Dataframe to see correlation\n",
    "corr = pd.DataFrame()\n",
    "\n",
    "# Passing score data to corr dataframe\n",
    "corr['SVM_score'] = score['score']\n",
    "corr['KM_cluster_distance'] = k_anomaly['0']\n",
    "corr['AutoEnc_loss'] = enc_loss['Loss_mae']\n",
    "\n",
    "# Seeing correlation\n",
    "corr.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading data and passing it to dataframe again\n",
    "k_anomaly = pd.read_csv('../input/created/KM_Distance.csv')\n",
    "score = pd.read_csv('../input/created/SVM_Score.csv')\n",
    "enc_loss = pd.read__csv('../input/created/AutoEncoder_loss.csv')\n",
    "\n",
    "corr = pd.DataFrame()\n",
    "corr['SVM_score'] = score['score']\n",
    "corr['KM_cluster_distance'] = k_anomaly['0']\n",
    "corr['AutoEnc_loss'] = enc_loss['Loss_mae']\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "\n",
    "# Plot size\n",
    "plt.subplots(figsize(15, 7))\n",
    "\n",
    "# Scatter plot of SVM score\n",
    "plt.plot(corr.index, corr['SVM_score'], 'g.', markersize = 1, label = 'OCSVM_score')\n",
    "# Plotting moving mean of 1000 data points\n",
    "plt.plot(corr.index, corr['SVM_score'].rolling(1000).mean(), 'r', markersize = 1, label = 'Moving Mean')\n",
    "# Legend\n",
    "plt.legend(loc = 'upper right')\n",
    "# Show\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(figsize(15, 7))\n",
    "plt.plot(corr.index, corr['KM_cluster_distance'], 'g.', markersize = 1, label = 'KM_cluster_distance')\n",
    "plt.plot(corr.index, corr['KM_cluster_distance'].rolling(1000).mean(), 'r', markersize = 1, label = 'Moving Mean')\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(figsize(15, 7))\n",
    "plt.plot(corr.index, corr['AutoEnc_loss'], 'g.', markersize = 1, label = 'AutoEnc_loss')\n",
    "plt.plot(corr.index, corr['AutoEnc_loss'].rolling(1000).mean(), 'r', markersize = 1, label = 'Moving Mean')\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_anomaly = pd.read_csv('../input/created/KM_Distance.csv')\n",
    "score = pd.read_csv('../input/created/SVM_Score.csv')\n",
    "enc_loss = pd.read__csv('../input/created/AutoEncoder_loss.csv')\n",
    "\n",
    "corr = pd.DataFrame()\n",
    "corr['SVM_score'] = score['score']\n",
    "corr['KM_cluster_distance'] = k_anomaly['0']\n",
    "corr['AutoEnc_loss'] = enc_loss['Loss_mae']\n",
    "\n",
    "# Plot size\n",
    "plt.subplots(figsize(10, 7))\n",
    "# Hist plot of first 160.000 rows, 15 bins\n",
    "sns.displot(corr['SVM_score'].head(160000), bins = 15)\n",
    "# Show\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(figsize(10, 7))\n",
    "sns.displot(corr['KM_cluster_distance'].head(160000), bins = 15)\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(figsize(10, 7))\n",
    "sns.displot(corr['AutoEnc_loss'].head(160000), bins = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_anomaly = pd.read_csv('../input/created/KM_Distance.csv')\n",
    "score = pd.read_csv('../input/created/SVM_Score.csv')\n",
    "enc_loss = pd.read__csv('../input/created/AutoEncoder_loss.csv')\n",
    "\n",
    "corr = pd.DataFrame()\n",
    "corr['SVM_score'] = score['score']\n",
    "corr['KM_cluster_distance'] = k_anomaly['0']\n",
    "corr['AutoEnc_loss'] = enc_loss['Loss_mae']\n",
    "\n",
    "\n",
    "plt.subplots(figsize(10, 7))\n",
    "sns.displot(corr['SVM_score'], bins = 15)\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(figsize(10, 7))\n",
    "sns.displot(corr['KM_cluster_distance'], bins = 15)\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(figsize(10, 7))\n",
    "sns.displot(corr['AutoEnc_loss'], bins = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_anomaly = pd.read_csv('../input/created/KM_Distance.csv')\n",
    "score = pd.read_csv('../input/created/SVM_Score.csv')\n",
    "enc_loss = pd.read__csv('../input/created/AutoEncoder_loss.csv')\n",
    "\n",
    "corr = pd.DataFrame()\n",
    "corr['SVM_score'] = score['score']\n",
    "corr['KM_cluster_distance'] = k_anomaly['0']\n",
    "corr['AutoEnc_loss'] = enc_loss['Loss_mae']\n",
    "\n",
    "# Creating an array for the thresholds to be plotted over the entire dataset\n",
    "lower_threshold = np.full((corr['SVM_score'].size, 1), 0)\n",
    "upper_threshold = np.full((corr['SVM_score'].size, 1), 18000)\n",
    "high_density_threshold = np.full((corr['SVM_score'].size, 1), 13250)\n",
    "\n",
    "# Plot size\n",
    "plt.subplots(figsize(15, 7))\n",
    "\n",
    "# Score plot\n",
    "plt.plot(corr.index, corr['SVM_score'], 'k', markersize = 1, label = 'OCSVM_score')\n",
    "# Moving mean plot\n",
    "plt.plot(corr.index, corr['SVM_score'].rolling(100).mean(), 'r', markersize = 1, label = 'Moving mean')\n",
    "# Threshold plots\n",
    "plt.plot(corr.index, lower_threshold, label = 'Lower Threshold')\n",
    "plt.plot(corr.index, upper_threshold, label = 'Upper Threshold')\n",
    "plt.plot(corr.index, high_density_threshold, label = 'Highest Density')\n",
    "plt.legend(loc = 'upper right')\n",
    "# Show\n",
    "plt.show()\n",
    "\n",
    "\n",
    "lower_threshold = np.full((corr['KM_cluster_distance'].size, 1), 1.2)\n",
    "upper_threshold = np.full((corr['KM_cluster_distance'].size, 1), 17.5)\n",
    "high_desity_threshold = np.full((corr['KM_cluster_distance'].size, 1), 2.5)\n",
    "\n",
    "plt.subplots(figsize(15, 7))\n",
    "\n",
    "plt.plot(corr.index, corr['KM_cluster_distance'], 'k', markersize = 1, label = 'KM_cluster_distance')\n",
    "plt.plot(corr.index, corr['KM_cluster_distance'].rolling(100).mean(), 'r', markersize = 1, label = 'Moving mean')\n",
    "plt.plot(corr.index, lower_threshold, label = 'Lower Threshold')\n",
    "plt.plot(corr.index, upper_threshold, label = 'Upper Threshold')\n",
    "plt.plot(corr.index, high_density_threshold, label = 'Highest Density')\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "lower_threshold = np.full((corr['AutoEnc_loss'].size, 1), 0)\n",
    "upper_threshold = np.full((corr['AutoEnc_loss'].size, 1), 0.1)\n",
    "high_desity_threshold = np.full((corr['AutoEnc_loss'].size, 1), 0.05)\n",
    "\n",
    "plt.subplots(figsize(15, 7))\n",
    "\n",
    "plt.plot(corr.index, corr['AutoEnc_loss'], 'k', markersize = 1, label = 'AutoEnc_loss')\n",
    "plt.plot(corr.index, corr['AutoEnc_loss'].rolling(100).mean(), 'r', markersize = 1, label = 'Moving mean')\n",
    "plt.plot(corr.index, lower_threshold, label = 'Lower Threshold')\n",
    "plt.plot(corr.index, upper_threshold, label = 'Upper Threshold')\n",
    "plt.plot(corr.index, high_density_threshold, label = 'Highest Density')\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_anomaly = pd.read_csv('../input/created/KM_Distance.csv')\n",
    "score = pd.read_csv('../input/created/SVM_Score.csv')\n",
    "enc_loss = pd.read__csv('../input/created/AutoEncoder_loss.csv')\n",
    "\n",
    "corr = pd.DataFrame()\n",
    "corr['SVM_score'] = score['score']\n",
    "corr['KM_cluster_distance'] = k_anomaly['0']\n",
    "corr['AutoEnc_loss'] = enc_loss['Loss_mae']\n",
    "\n",
    "\n",
    "plt.subplots(figsize = (15, 7))\n",
    "\n",
    "plt.plot(corr['KM_cluster_distance'], ['SVM_score'], 'b.', markersize = 1)\n",
    "plt.xlable('KM')\n",
    "plt.ylable('SVM')\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(figsize = (15, 7))\n",
    "\n",
    "plt.plot(corr['AutoEnc_loss'], ['SVM_score'], 'b.', markersize = 1)\n",
    "plt.xlable('Encoder')\n",
    "plt.ylable('SVM')\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(figsize = (15, 7))\n",
    "\n",
    "plt.plot(corr['AutoEnc_loss'], ['KM_cluster_distance'], 'b.', markersize = 1)\n",
    "plt.xlable('Encoder')\n",
    "plt.ylable('KM')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_anomaly = pd.read_csv('../input/created/KM_Distance.csv')\n",
    "score = pd.read_csv('../input/created/SVM_Score.csv')\n",
    "enc_loss = pd.read__csv('../input/created/AutoEncoder_loss.csv')\n",
    "\n",
    "corr = pd.DataFrame()\n",
    "corr['SVM_score'] = score['score']\n",
    "corr['KM_cluster_distance'] = k_anomaly['0']\n",
    "corr['AutoEnc_loss'] = enc_loss['Loss_mae']\n",
    "\n",
    "main_df = pd.read_csv('../input/datasetone-year-compiledcsv/One_year_compiled.csv')\n",
    "\n",
    "\n",
    "# Passing encoder loss to main dataframe, to make it easier to seperate by month\n",
    "main_df['AutoEnc_loss'] = corr['AutoEnc_loss']\n",
    "\n",
    "# Getting list of months\n",
    "months = main_df['month'].dropna().unique()\n",
    "\n",
    "# Looping throuth every month\n",
    "for month in months:\n",
    "    # Grabbing the slice of the dataframe for each month\n",
    "    month_df = main_df.groupby('month').get_group(month)\n",
    "    \n",
    "    # Array Thresholds\n",
    "    upper_threshold = np.full((month_df['AutoEnc_loss'].size, 1), 0.1)\n",
    "    high_density_threshold = np.full((month_df['AutoEnc_loss'].size, 1), 0.05)\n",
    "    \n",
    "    # Plot\n",
    "    plt.subplots(figsize = (15, 7))\n",
    "    plt.plot(month_df.index, month_df['AutoEnc_loss'], label = f'AutoEnc_loss month_{month}')\n",
    "    plt.plot(month_df.index, upper_threshold, label = 'Upper Threshold')\n",
    "    plt.plot(month_df.index, high_density_threshold, label = 'Highest Density')\n",
    "    plt.legend(loc = 'upper right')\n",
    "    plt.ylim(0, 1.3)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_anomaly = pd.read_csv('../input/created/KM_Distance.csv')\n",
    "score = pd.read_csv('../input/created/SVM_Score.csv')\n",
    "enc_loss = pd.read__csv('../input/created/AutoEncoder_loss.csv')\n",
    "\n",
    "corr = pd.DataFrame()\n",
    "corr['SVM_score'] = score['score']\n",
    "corr['KM_cluster_distance'] = k_anomaly['0']\n",
    "corr['AutoEnc_loss'] = enc_loss['Loss_mae']\n",
    "\n",
    "main_df = pd.read_csv('../input/datasetone-year-compiledcsv/One_year_compiled.csv')\n",
    "\n",
    "main_df['AutoEnc_loss'] = corr['AutoEnc_loss']\n",
    "\n",
    "months = main_df['month'].dropna().unique()\n",
    "\n",
    "for month in months:\n",
    "    month_df = main_df.groupby('month').get_group(month)\n",
    "    \n",
    "    plt.subplots(figsize = (15, 7))\n",
    "    sns.displot((month_df['AutoEnc_loss']), bins = 15).set_title(f'Month {month} Loss Distribution')\n",
    "    # X axis limits\n",
    "    plt.xlim([-1.2, 1.2])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k_anomaly = pd.read_csv('../input/created/KM_Distance.csv')\n",
    "score = pd.read_csv('../input/created/SVM_Score.csv')\n",
    "enc_loss = pd.read__csv('../input/created/AutoEncoder_loss.csv')\n",
    "\n",
    "corr = pd.DataFrame()\n",
    "corr['SVM_score'] = score['score']\n",
    "corr['KM_cluster_distance'] = k_anomaly['0']\n",
    "corr['AutoEnc_loss'] = enc_loss['Loss_mae']\n",
    "\n",
    "main_df = pd.read_csv('../input/datasetone-year-compiledcsv/One_year_compiled.csv')\n",
    "\n",
    "main_df['AutoEnc_loss'] = corr['AutoEnc_loss']\n",
    "\n",
    "months = main_df['month'].dropna().unique()\n",
    "\n",
    "for month in months:\n",
    "    month_df = main_df.groupby('month').get_group(month)\n",
    "    kurt = (month_df['AutoEnc_loss']).kurtosis()\n",
    "    print(f'Month {month} kurtosis = {kurt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preparing data for training\n",
    "main_df = pd.read_csv('../input/datasetone-year-compiledcsv/One_year_compiled.csv')\n",
    "main_df = main_df.drop(['day', 'hour', 'sample_Number', 'month', 'timestamp'], axis = 1)\n",
    "main_df = handle_non_numeric(main_df)\n",
    "X = main_df\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns = X.columns, index = X.index)\n",
    "\n",
    "X = preprocessing.scale(X)\n",
    "\n",
    "train_percentage = 0.15\n",
    "train_size = int(len(main_df.index) * train_percentage)\n",
    "\n",
    "X_train = X[:train_size]\n",
    "#-----------------------------------------------------------------------------------------\n",
    "\n",
    "# Seed for random batch validation and training\n",
    "seed(10)\n",
    "\n",
    "#Elu activation function\n",
    "act_func = 'elu'\n",
    "\n",
    "# Input layer\n",
    "model = Sequential()\n",
    "\n",
    "# First hidden layer, connected to input vector X\n",
    "model.add(Dense(50, activation = act_func,\n",
    "               kernel_initializer = 'glorot_uniform',\n",
    "               kernel_regularizer = regularizers.l2(0.0),\n",
    "               input_shape = (X_train.shape[1],)\n",
    "               )\n",
    "         )\n",
    "# Second hidden layer\n",
    "model.add(Dense(10, activation = act_func,\n",
    "               kernel_initializer = 'glorot_uniform'))\n",
    "# Third hidden layer\n",
    "model.add(Dense(50, activation = act_func,\n",
    "               kernel_initializer = 'glorot_uniform'))\n",
    "\n",
    "# Input layer\n",
    "model.add(Dense(X_train.shape[1],\n",
    "               kernel_initializer = 'glorot_uniform'))\n",
    "\n",
    "# Loss function and Optimizer choice\n",
    "model.compile(loss = 'mse', optimizer = 'adam')\n",
    "\n",
    "# Train model for 100 epochs, batch size of 10\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 200\n",
    "\n",
    "# Grabbing validation and training loss over epochs\n",
    "history = model.fit(np.array(X_train), np.array(X_train),\n",
    "                   batch_size = BATCH_SIZE,\n",
    "                   epochs = NUM_EPOCHS,\n",
    "                   validation_split = 0.1,\n",
    "                   verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predicting and passing prediction to dataframe\n",
    "X_pred = model.predict(np.array(X))\n",
    "X_pred = pd.DataFrame(X_pred, columns = main_df.columns)\n",
    "X_pred.index = pd.DataFrame(main.df).index\n",
    "\n",
    "# Passing X from an array to a dataframe\n",
    "X = pd.DataFrame(X, columns = main.df_columns)\n",
    "X.index = pd.DataFrame(main_df).index\n",
    "\n",
    "# Dataframe where all the loss per columns will go\n",
    "loss_df = pd.DataFrame()\n",
    "\n",
    "# Dropping mode as it can't logically contribute to degredation\n",
    "main_df.drop('mode', axis = 1, inplace = True)\n",
    "\n",
    "# Iterating throuth columns\n",
    "for column in main_df.columns:\n",
    "    # Getting the loss of the predection for that column\n",
    "    loss_df[f'{column}'] = (X_pred[f'{column}'] - X[f'{column}']).abs()\n",
    "    \n",
    "    # Plotting the loss\n",
    "    plt.subplots(figsize = (15, 7))\n",
    "    plt.plot(loss_df.index, loss_df[f'{column}'], lable = f'{column} loss')\n",
    "    plt.legend(loc = 'upper right')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "# Saving loss Dataframe\n",
    "loss_df.to_csv('AutoEncoder_loss_p_column.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stfmax_df = pd.read_csv('../input/created/AutoEncoder_loss_p_column.csv', index_col = 0)\n",
    "sftmax_df = softmax(sftmax_df, axis = 1)\n",
    "sftmax_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for column in sftmax_df.columns:\n",
    "    \n",
    "    plt.subplots(figsize = (15, 7))\n",
    "    plt.plot(sftmax_df.index, sftmax_df[f'{column}'], label = f'{column} loss')\n",
    "    plt.legend(loc = 'upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (15, 7))\n",
    "\n",
    "# Lables for stackbar plot\n",
    "df_label = ['Torque', 'Cut lag', 'Cut speed', 'Cut position', 'Film position', 'Film speed', 'Film lag', 'VAX']\n",
    "\n",
    "# Stackbar plot\n",
    "plt.stackplot(sftmax_df.index, sftmax_df['pCut::Motor_Torque'],\n",
    "             sftmax_df['pCut::CTRL_Position_controller::Lag_error'],\n",
    "             sftmax_df['pCut::CTRL_Position_controller::Actual_speed'],\n",
    "             sftmax_df['pCut::CTRL_Position_controller::Actual_position'],\n",
    "             sftmax_df['pSvolFilm::CTRL_Position_controller::Actual_position'],\n",
    "             sftmax_df['pSvolFilm::CTRL_Position_controller::Actual_speed'],\n",
    "             sftmax_df['pSvolFilm::CTRL_Position_controller::Lag_error'],\n",
    "             sftmax_df['pSpintor::VAX_speed'],\n",
    "             labels = df_label)\n",
    "\n",
    "plt.legend(loc = 'upper center', ncol = 8)\n",
    "\n",
    "plt.ylim(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (15, 7))\n",
    "\n",
    "# Lables for stackbar plot\n",
    "df_label = ['Torque', 'Cut lag', 'Cut speed', 'Cut position', 'Film position', 'Film speed', 'Film lag', 'VAX']\n",
    "\n",
    "# Grabbing the slice where the larger anomaly is\n",
    "sftmax_df = sftmax_df[400000 : 600000]\n",
    "\n",
    "# Stackbar plot\n",
    "plt.stackplot(sftmax_df.index, sftmax_df['pCut::Motor_Torque'],\n",
    "             sftmax_df['pCut::CTRL_Position_controller::Lag_error'],\n",
    "             sftmax_df['pCut::CTRL_Position_controller::Actual_speed'],\n",
    "             sftmax_df['pCut::CTRL_Position_controller::Actual_position'],\n",
    "             sftmax_df['pSvolFilm::CTRL_Position_controller::Actual_position'],\n",
    "             sftmax_df['pSvolFilm::CTRL_Position_controller::Actual_speed'],\n",
    "             sftmax_df['pSvolFilm::CTRL_Position_controller::Lag_error'],\n",
    "             sftmax_df['pSpintor::VAX_speed'],\n",
    "             labels = df_label)\n",
    "\n",
    "plt.legend(loc = 'upper center', ncol = 8)\n",
    "\n",
    "plt.ylim(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for column in sftmax_df.columns:\n",
    "    \n",
    "    plt.subplots(figsize = (15, 7))\n",
    "    sns.displot((sftmax_df[f'{column}']), bins = 15).set_title(f'Contribution Distribution')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
